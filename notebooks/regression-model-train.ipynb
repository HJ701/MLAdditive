{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a79138",
   "metadata": {},
   "source": [
    "# Regression Model Training Pipeline\n",
    "This notebook is converted from a Python training script. It includes steps to load the preprocessed data, train multiple regression models, evaluate them, and save the results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Import libraries and define helper functions\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# === Paths ===\n",
    "INPUT_PATH = '/Users/hj/MLAdditive/data/preprocessed.csv'\n",
    "BASE_RESULTS_DIR = '/Users/hj/MLAdditive/results'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274eae3",
   "metadata": {},
   "source": [
    "## Function: `load_data`\n",
    "Loads preprocessed dataset from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e91eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0a9d2",
   "metadata": {},
   "source": [
    "## Function: `evaluate_model`\n",
    "Evaluates the regression model using key performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46671934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    print(f\"\\nüß† Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Output directory\n",
    "    out_dir = os.path.join(BASE_RESULTS_DIR, name.replace(\" \", \"_\").lower())\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model, os.path.join(out_dir, f\"{name}_model.pkl\"))\n",
    "\n",
    "    # Save metrics\n",
    "    results_df = pd.DataFrame({\n",
    "        'Metric': ['MAE', 'MSE', 'RMSE', 'R2'],\n",
    "        'Value': [mae, mse, rmse, r2]\n",
    "    })\n",
    "    results_df.to_csv(os.path.join(out_dir, 'evaluation_metrics.csv'), index=False)\n",
    "\n",
    "    print(f\"üìâ {name} Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Plot: Actual vs Predicted\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Lifetime_years')\n",
    "    plt.ylabel('Predicted Lifetime_years')\n",
    "    plt.title(f'{name}: Actual vs Predicted')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, 'actual_vs_predicted.png'))\n",
    "\n",
    "    # Plot: Residual Distribution\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure()\n",
    "    plt.hist(residuals, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{name}: Residuals Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, 'residuals_distribution.png'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aae658",
   "metadata": {},
   "source": [
    "## Function: `train_all_models`\n",
    "This function is a core component of the model training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1938dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(df):\n",
    "    X = df.drop('Lifetime_years', axis=1)\n",
    "    y = df['Lifetime_years']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"Ridge Regression\": GridSearchCV(Ridge(), param_grid={'alpha': [0.1, 1.0, 10.0, 100.0]}, cv=5, scoring='neg_mean_squared_error'),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        evaluate_model(name, model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data(INPUT_PATH)\n",
    "    train_all_models(df)\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "# === Paths ===\n",
    "INPUT_PATH = '/Users/hj/MLAdditive/data/preprocessed.csv'\n",
    "BASE_RESULTS_DIR = '/Users/hj/MLAdditive/results/regression'\n",
    "\n",
    "# === Custom Scorers for CV ===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87dcc2",
   "metadata": {},
   "source": [
    "## Function: `rmse_scorer`\n",
    "This function is a core component of the model training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_score = make_scorer(rmse_scorer, greater_is_better=False)\n",
    "mae_score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc773c",
   "metadata": {},
   "source": [
    "## Function: `load_data`\n",
    "Loads preprocessed dataset from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07708c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f16f97",
   "metadata": {},
   "source": [
    "## Function: `evaluate_model`\n",
    "Evaluates the regression model using key performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3335f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test, X_full, y_full):\n",
    "    print(f\"\\nüß† Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Eval metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    out_dir = os.path.join(BASE_RESULTS_DIR, name.replace(\" \", \"_\").lower())\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model, os.path.join(out_dir, f\"{name}_model.pkl\"))\n",
    "\n",
    "    # Save test split metrics\n",
    "    test_metrics = pd.DataFrame({\n",
    "        'Metric': ['MAE', 'MSE', 'RMSE', 'R2'],\n",
    "        'Value': [mae, mse, rmse, r2]\n",
    "    })\n",
    "    test_metrics.to_csv(os.path.join(out_dir, 'evaluation_metrics.csv'), index=False)\n",
    "\n",
    "    print(f\"üìâ {name} Test Results:\\n{test_metrics.to_string(index=False)}\")\n",
    "\n",
    "    # Plot: Actual vs Predicted\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Lifetime_years')\n",
    "    plt.ylabel('Predicted Lifetime_years')\n",
    "    plt.title(f'{name}: Actual vs Predicted')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, 'actual_vs_predicted.png'))\n",
    "\n",
    "    # Plot: Residual Distribution\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure()\n",
    "    plt.hist(residuals, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{name}: Residuals Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, 'residuals_distribution.png'))\n",
    "\n",
    "    # === Cross-validation ===\n",
    "    print(f\"üîÅ Running 5-Fold Cross-Validation for {name}...\")\n",
    "\n",
    "    cv_r2 = cross_val_score(model, X_full, y_full, cv=5, scoring='r2')\n",
    "    cv_mae = -cross_val_score(model, X_full, y_full, cv=5, scoring=mae_score)\n",
    "    cv_rmse = -cross_val_score(model, X_full, y_full, cv=5, scoring=rmse_score)\n",
    "\n",
    "    cv_df = pd.DataFrame({\n",
    "        'Metric': ['CV_R2', 'CV_MAE', 'CV_RMSE'],\n",
    "        'Mean': [cv_r2.mean(), cv_mae.mean(), cv_rmse.mean()],\n",
    "        'Std': [cv_r2.std(), cv_mae.std(), cv_rmse.std()]\n",
    "    })\n",
    "\n",
    "    cv_df.to_csv(os.path.join(out_dir, 'crossval_metrics.csv'), index=False)\n",
    "\n",
    "    print(f\"üìä {name} Cross-Validation (5-Fold) Summary:\\n{cv_df.to_string(index=False)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd67fdf",
   "metadata": {},
   "source": [
    "## Function: `train_all_models`\n",
    "This function is a core component of the model training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1150f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(df):\n",
    "    X = df.drop('Lifetime_years', axis=1)\n",
    "    y = df['Lifetime_years']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"Ridge Regression\": GridSearchCV(Ridge(), param_grid={'alpha': [0.1, 1.0, 10.0, 100.0]}, cv=5, scoring='neg_mean_squared_error'),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        evaluate_model(name, model, X_train, X_test, y_train, y_test, X, y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data(INPUT_PATH)\n",
    "    train_all_models(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9ec1b",
   "metadata": {},
   "source": [
    "## Model Training Execution\n",
    "This section runs the full pipeline using selected models and saves the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     df = load_data(INPUT_PATH)\n",
    "#     train_all_models(df)\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "# === Paths ===\n",
    "INPUT_PATH = '/Users/hj/MLAdditive/data/preprocessed.csv'\n",
    "BASE_RESULTS_DIR = '/Users/hj/MLAdditive/results/regression'\n",
    "\n",
    "# === Custom Scorers for CV ===\n",
    "def rmse_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_score = make_scorer(rmse_scorer, greater_is_better=False)\n",
    "mae_score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test, X_full, y_full):\n",
    "    print(f\"\\nüß† Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Eval metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    out_dir = os.path.join(BASE_RESULTS_DIR, name.replace(\" \", \"_\").lower())\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model, os.path.join(out_dir, f\"{name}_model.pkl\"))\n",
    "\n",
    "    # Save test split metrics\n",
    "    test_metrics = pd.DataFrame({\n",
    "        'Metric': ['MAE', 'MSE', 'RMSE', 'R2'],\n",
    "        'Value': [mae, mse, rmse, r2]\n",
    "    })\n",
    "    test_metrics.to_csv(os.path.join(out_dir, 'evaluation_metrics.csv'), index=False)\n",
    "\n",
    "    print(f\"üìâ {name} Test Results:\\n{test_metrics.to_string(index=False)}\")\n",
    "\n",
    "    # Plot: Actual vs Predicted\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Lifetime_years')\n",
    "    plt.ylabel('Predicted Lifetime_years')\n",
    "    plt.title(f'{name}: Actual vs Predicted')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, 'actual_vs_predicted.png'))\n",
    "\n",
    "    # Plot: Residual Distribution\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure()\n",
    "    plt.hist(residuals, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{name}: Residuals Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, 'residuals_distribution.png'))\n",
    "\n",
    "    # === Cross-validation ===\n",
    "    print(f\"üîÅ Running 5-Fold Cross-Validation for {name}...\")\n",
    "\n",
    "    cv_r2 = cross_val_score(model, X_full, y_full, cv=5, scoring='r2')\n",
    "    cv_mae = -cross_val_score(model, X_full, y_full, cv=5, scoring=mae_score)\n",
    "    cv_rmse = -cross_val_score(model, X_full, y_full, cv=5, scoring=rmse_score)\n",
    "\n",
    "    cv_df = pd.DataFrame({\n",
    "        'Metric': ['CV_R2', 'CV_MAE', 'CV_RMSE'],\n",
    "        'Mean': [cv_r2.mean(), cv_mae.mean(), cv_rmse.mean()],\n",
    "        'Std': [cv_r2.std(), cv_mae.std(), cv_rmse.std()]\n",
    "    })\n",
    "\n",
    "    cv_df.to_csv(os.path.join(out_dir, 'crossval_metrics.csv'), index=False)\n",
    "\n",
    "    print(f\"üìä {name} Cross-Validation (5-Fold) Summary:\\n{cv_df.to_string(index=False)}\")\n",
    "\n",
    "def train_all_models(df):\n",
    "    X = df.drop('Lifetime_years', axis=1)\n",
    "    y = df['Lifetime_years']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"Ridge Regression\": GridSearchCV(Ridge(), param_grid={'alpha': [0.1, 1.0, 10.0, 100.0]}, cv=5, scoring='neg_mean_squared_error'),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        evaluate_model(name, model, X_train, X_test, y_train, y_test, X, y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
